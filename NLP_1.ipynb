{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8i4ZUaIqp_O"
   },
   "source": [
    "NLP_Homework_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TF3mcsseRnm7"
   },
   "source": [
    "1. Which of the following is not an application of NLP?\n",
    "a. Image labeling\n",
    "b. Poetry generation\n",
    "c. Sentimental analysis\n",
    "d. E-mail classification\n",
    "\n",
    "\n",
    "ans: (a) Image labeling is not an application of NLP.\n",
    "\n",
    "NLP deals with understanding and processing human language, while image labeling involves identifying objects or features within images.\n",
    "This is under the domain of computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX91w7xdqmoK"
   },
   "source": [
    "2. Which of the following is not an NLP task?\n",
    "   a. Tokenization\n",
    "   b. Stop word removal\n",
    "   c. Part-of-speech tagging\n",
    "   d. Image segmentation\n",
    "\n",
    "ans: (d) Image segmentation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tokenization: This is an NLP task that involves breaking down text into smaller units, such as words or phrases. Stop word removal: This is an NLP task where common words (like \"and,\" \"the,\" etc.) that are often removed from text to focus on more meaningful words are filtered out. Part-of-speech tagging: This NLP task involves identifying the part of speech (e.g., noun, verb, adjective) for each word in a sentence. Image segmentation is not an NLP task; it is a computer vision task that involves partitioning an image into multiple segments or regions, often to simplify the analysis or to identify objects within an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxnuyfQrrO4b"
   },
   "source": [
    "3. Which of the following is not a disadvantage of rule-based approaches for NLP?\n",
    "   a. Not flexible\n",
    "   b. Not scalable\n",
    "   c. Requires huge dataset\n",
    "   d. None of the above\n",
    "\n",
    "\n",
    "ans: (c) Requires huge dataset\n",
    "\n",
    "\n",
    "\n",
    "Not flexible: Rule-based approaches are often rigid and require manual updates to adapt to new patterns or variations in the language. Not scalable: Rule-based systems can become complex and unwieldy as the volume of data or the number of rules increases, making them difficult to scale. Requires huge dataset: This is not typically a disadvantage of rule-based approaches. In fact, rule-based systems do not require large datasets because they rely on explicitly defined rules rather than learning from data. They use predefined rules and patterns set by experts, so they are less dependent on large datasets compared to machine learning approaches. Therefore, requiring a huge dataset is not a disadvantage of rule-based approaches, making option c the correct choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSsH3RzqrjVA"
   },
   "source": [
    "4. What are the two major types of NLP approaches?\n",
    "\n",
    "\n",
    "ans: The first step in natural language processing (NLP) is data pre-processing, which is simply cleaning and organizing the data into a common format that the algorithm can understand. In another way, the goal of pre-processing text input is to format the content so that the model can comprehend it and use it to train itself to resemble human comprehension.\n",
    "\n",
    "The 2 main approaches of NLP can be split into two major groups: syntactic and semantic\n",
    "\n",
    "Syntactic approaches\n",
    "\n",
    "Anything syntax-specific can be found under this category:\n",
    "\n",
    "(1)Lemmatization: As one of the key techniques in NLP for data pre-processing, lemmatization is essentially reducing the word to its root word, also called a lemma. Take irregular comparatives and superlatives, for example. A lemmatization algorithm can identify that the root of less is little.\n",
    "\n",
    "(2)Stemming: Stemming, by contrast, although shares the definition with lemmatization holding up to the same word-reduction logic, would not spot the connection between less and little. It would just chop off one letter at a time, without getting to the essence of the word.\n",
    "\n",
    "(3)Segmentation: By breaking the word into smaller morphemes (units) morphological segmentation extends its applications to speech recognition, data retrieval, machine translation, etc.\n",
    "\n",
    "(4)Part-of-speech tagging: Dealing with the syntactic structure, part-of-speech tagging refers to analyzing and interpreting grammatical units of the words, be they nouns, verbs, adverbs, and so on.\n",
    "\n",
    "(5)Parsing: By and large, parsing also refers to the grammatical analysis of the provided sentence, except here, sentences are assigned a structure to reflect how sentence constituents are related to one another. This is the reason why parsing often results in a sentence-level parse tree.\n",
    "\n",
    "(6)Tokenization: By dividing a sentence into smaller parts or setting sentence boundaries, tokenization allows for easier parsing later on.\n",
    "\n",
    "Semantic approaches\n",
    "\n",
    "Albeit limited in number, semantic approaches are equally significant to natural language processing.\n",
    "\n",
    "(1)Named entity recognition: This method allows to label parts of the text into relevant groups, be they names, places objects, etc. Grammarly, Siri, Alexa, and Google Translate all use named entity recognition as part of natural language processing to understand textual data.\n",
    "\n",
    "(2)Word sense disambiguation: What word sense disambiguation does, humans do on an unconscious level. Some words do not have a single meaning and we, humans, fit them into the context without much effort. For a machine to be able to do the same, it has to identify which ‘sense’ or meaning of the word is triggered in a given context—all thanks to word sense disambiguation.\n",
    "\n",
    "(3)Natural language generation: As inferred, natural language generation (also NLG) is about using input data or databases to conduct semantic analysis and deliver human language text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GjjT9aBuMJ9"
   },
   "source": [
    "5. Use TextBlob to translate a sentence in English into French, Mandarin, and\n",
    "Hindi?\n",
    "a. Import TextBlob\n",
    "b. Languages are coded as ‘fr’, ‘zh-CN’, ‘hi’\n",
    "c. Translate the following sentence “Who knew translation could be fun” into\n",
    "French, Mandarin, and Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMqdIJ243ovh",
    "outputId": "58761230-9fa1-466e-c346-a563725144e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/shriaiswaryagorle/anaconda3/lib/python3.11/site-packages/pyBWMD-0.0.1-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: Textblob in /Users/shriaiswaryagorle/anaconda3/lib/python3.11/site-packages (0.18.0.post0)\r\n",
      "Requirement already satisfied: nltk>=3.8 in /Users/shriaiswaryagorle/anaconda3/lib/python3.11/site-packages (from Textblob) (3.8.1)\r\n",
      "Requirement already satisfied: click in /Users/shriaiswaryagorle/anaconda3/lib/python3.11/site-packages (from nltk>=3.8->Textblob) (8.0.4)\r\n",
      "Requirement already satisfied: joblib in /Users/shriaiswaryagorle/anaconda3/lib/python3.11/site-packages (from nltk>=3.8->Textblob) (1.2.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/shriaiswaryagorle/anaconda3/lib/python3.11/site-packages (from nltk>=3.8->Textblob) (2022.7.9)\r\n",
      "Requirement already satisfied: tqdm in /Users/shriaiswaryagorle/anaconda3/lib/python3.11/site-packages (from nltk>=3.8->Textblob) (4.65.0)\r\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TextBlob' object has no attribute 'translate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m blob \u001b[38;5;241m=\u001b[39m TextBlob(sentence)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Translate the sentence to French ('fr'), Mandarin ('zh-CN'), and Hindi ('hi') with from_lang='en'\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m translation_french \u001b[38;5;241m=\u001b[39m blob\u001b[38;5;241m.\u001b[39mtranslate(from_lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m translation_mandarin \u001b[38;5;241m=\u001b[39m blob\u001b[38;5;241m.\u001b[39mtranslate(from_lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzh-CN\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m translation_hindi \u001b[38;5;241m=\u001b[39m blob\u001b[38;5;241m.\u001b[39mtranslate(from_lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TextBlob' object has no attribute 'translate'"
     ]
    }
   ],
   "source": [
    "# Install all the required libraries\n",
    "!pip install Textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# The sentence to translate\n",
    "sentence = \"Who knew translation could be fun\"\n",
    "\n",
    "# Create a TextBlob object for the sentence\n",
    "blob = TextBlob(sentence)\n",
    "\n",
    "# Translate the sentence to French ('fr'), Mandarin ('zh-CN'), and Hindi ('hi') with from_lang='en'\n",
    "translation_french = blob.translate(from_lang='en', to='fr')\n",
    "translation_mandarin = blob.translate(from_lang='en', to='zh-CN')\n",
    "translation_hindi = blob.translate(from_lang='en', to='hi')\n",
    "\n",
    "# Print the translations\n",
    "print(f\"French Translation: {translation_french}\")\n",
    "print(f\"Mandarin Translation: {translation_mandarin}\")\n",
    "print(f\"Hindi Translation: {translation_hindi}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
